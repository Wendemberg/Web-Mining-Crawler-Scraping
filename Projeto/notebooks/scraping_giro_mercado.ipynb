{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Web Scraping - Money Times (Giro do Mercado)\n\n## Informações do Projeto\n\n| Aspecto | Detalhes |\n|--------|----------|\n| **Linguagem** | Python 3.13.7 |\n| **Coleta** | Requests + BeautifulSoup + Selenium |\n| **Banco Analítico** | DuckDB |\n| **Ambiente** | Jupyter Notebook |\n\n## Requirements\n```\npython-dateutil==2.8.2\nrequests==2.31.0\nduckdb==0.9.2\nyfinance==0.2.32\nnumpy==1.26.2\npandas==2.1.3\nlxml==4.9.3\nwebdriver-manager==4.0.1\nbeautifulsoup4==4.12.2\nselenium==4.15.2\n```\n\n## Objetivo\nColetar **110 notícias válidas** da seção Giro do Mercado do Money Times.\n\n## Dados Coletados\n- **Título** da notícia\n- **Data/Hora** de publicação\n- **URL** completa da notícia\n- **Lead/Primeiro parágrafo** da notícia\n\n## URL Base\nhttps://www.moneytimes.com.br/tag/giro-do-mercado/"
  },
  {
   "cell_type": "markdown",
   "source": "## Sobre o Banco de Dados (DuckDB)\n\n### Objetivo\nArmazenar e analisar dados de notícias do Giro do Mercado junto com informações de preços históricos do Bitcoin, permitindo análises correlacionadas entre eventos de mercado e variações de criptomoedasmoedas.\n\n### Fonte de Dados\n- **Notícias**: Money Times (https://www.moneytimes.com.br/tag/giro-do-mercado/)\n- **Preços Bitcoin**: yfinance API (BTC-USD) - Últimos 6 meses de histórico\n- **Período**: Últimos 6 meses de dados estruturados e históricos\n\n### Estrutura do Banco\n\n**TB_NOTICIAS** (Tabela Principal)\n- `id_noticia`: Identificador único\n- `titulo`: Título completo da notícia\n- `url`: Link direto para a matéria\n- `lead`: Primeiro parágrafo/resumo\n- `data_publicacao`: Data/hora de publicação (convertida)\n- `data_extracao`: Data/hora da coleta dos dados\n- `hash_noticia`: Hash único para deduplicação\n- `ano`, `mes`, `dia`: Dimensões temporais\n\n**TB_METRICAS** (Análise de Conteúdo)\n- `id_noticia`: Foreign key para tb_noticias\n- `comprimento_titulo`: Número de caracteres do título\n- `comprimento_lead`: Número de caracteres do lead\n- `num_palavras_titulo`: Contagem de palavras no título\n- `num_palavras_lead`: Contagem de palavras no lead\n- `tem_url`: Flag indicando presença de URL\n\n**TB_ATIVO** (Dados de Ativos + Bitcoin)\n- `id_ativo`: Identificador do ativo\n- `id_noticia`: Foreign key para tb_noticias\n- `titulo`: Título da notícia\n- `url`: URL da notícia\n- `data_publicacao`: Data da notícia\n- `bitcoin`: **Preço de fechamento do Bitcoin (BTC-USD)** para a data correspondente\n- `ano`, `mes`, `dia`: Dimensões temporais\n\n**TB_AUDITORIA** (Rastreabilidade)\n- `id_execucao`: Identificador único da execução\n- `data_execucao`: Timestamp da execução\n- `total_noticias`: Quantidade de notícias coletadas\n- `total_metricas`: Quantidade de métricas calculadas\n- `total_ativo`: Quantidade de registros em tb_ativo\n- `bitcoin_preenchidos`: Quantidade de preços Bitcoin preenchidos\n- `origem_dados`: Fonte dos dados\n- `fonte_bitcoin`: Fonte dos dados de Bitcoin\n- `status`: Status da execução (SUCESSO/ERRO)\n\n### Recursos Principais\n- ✅ **Deduplicação robusta** com hash SHA256\n- ✅ **Validação de dados** (URLs, datas, títulos)\n- ✅ **Enriquecimento de dados** com preços históricos de Bitcoin\n- ✅ **Relacionamentos** entre tabelas via foreign keys\n- ✅ **Rastreabilidade** completa de execuções\n- ✅ **Otimizado para análise** com DuckDB (OLAP)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação de Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium beautifulsoup4 pandas webdriver-manager lxml -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\teste\\venv\\Scripts\\python.exe\n",
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuração do WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Função setup_driver() criada!\n"
     ]
    }
   ],
   "source": [
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configura e retorna um WebDriver do Chrome otimizado para scraping.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    \n",
    "    # Inicializa o driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "print(\"Função setup_driver() criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Função para Extrair Notícias de uma Página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Função extrair_noticias_pagina() criada!\n"
     ]
    }
   ],
   "source": [
    "def extrair_noticias_pagina(driver, url):\n",
    "    \"\"\"\n",
    "    Extrai todas as notícias de uma página do Giro do Mercado.\n",
    "    \n",
    "    Estrutura esperada:\n",
    "    - class=\"news-item\" (div principal)\n",
    "    - class=\"news-item__title\" (título)\n",
    "    - class=\"news-item__content\" (lead/parágrafo)\n",
    "    - class=\"date\" (data/hora)\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver do Selenium\n",
    "        url: URL da página a ser analisada\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dicionários com as notícias\n",
    "    \"\"\"\n",
    "    noticias = []\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # MÉTODO 1: Tenta usar a estrutura news-item\n",
    "        news_items = soup.find_all(class_='news-item')\n",
    "        \n",
    "        if news_items:\n",
    "            print(f\"  Encontrados {len(news_items)} elementos 'news-item'\")\n",
    "            \n",
    "            for item in news_items:\n",
    "                try:\n",
    "                    # Extrai título\n",
    "                    titulo_elem = item.find(class_='news-item__title')\n",
    "                    if not titulo_elem:\n",
    "                        titulo_elem = item.find(['h2', 'h3', 'h4'])\n",
    "                    \n",
    "                    if not titulo_elem:\n",
    "                        continue\n",
    "                    \n",
    "                    titulo = titulo_elem.get_text(strip=True)\n",
    "                    \n",
    "                    # Extrai link\n",
    "                    link_elem = item.find('a', href=True)\n",
    "                    url_noticia = ''\n",
    "                    if link_elem:\n",
    "                        url_noticia = link_elem['href']\n",
    "                        if url_noticia.startswith('/'):\n",
    "                            url_noticia = 'https://www.moneytimes.com.br' + url_noticia\n",
    "                    \n",
    "                    # Extrai data/hora\n",
    "                    data_elem = item.find(class_='date')\n",
    "                    if not data_elem:\n",
    "                        data_elem = item.find('time')\n",
    "                    data_hora = data_elem.get_text(strip=True) if data_elem else ''\n",
    "                    \n",
    "                    # Extrai lead\n",
    "                    lead_elem = item.find(class_='news-item__content')\n",
    "                    if not lead_elem:\n",
    "                        lead_elem = item.find('p')\n",
    "                    lead = lead_elem.get_text(strip=True) if lead_elem else ''\n",
    "                    \n",
    "                    noticias.append({\n",
    "                        'titulo': titulo,\n",
    "                        'data_hora': data_hora,\n",
    "                        'url': url_noticia,\n",
    "                        'lead': lead,\n",
    "                        'data_extracao': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # MÉTODO 2: Se não encontrou, usa abordagem alternativa\n",
    "        if not noticias:\n",
    "            print(f\"  Usando método alternativo de extração...\")\n",
    "            \n",
    "            # Busca artigos ou divs que contenham notícias\n",
    "            containers = soup.find_all(['article', 'div'], class_=lambda x: x and ('post' in str(x).lower() or 'article' in str(x).lower() or 'news' in str(x).lower()))\n",
    "            \n",
    "            if not containers:\n",
    "                containers = soup.find_all('a', href=True)\n",
    "            \n",
    "            for container in containers:\n",
    "                try:\n",
    "                    # Pega título\n",
    "                    titulo_elem = container.find(['h2', 'h3', 'h4'])\n",
    "                    if not titulo_elem:\n",
    "                        if container.name == 'a':\n",
    "                            texto = container.get_text(strip=True)\n",
    "                            if len(texto) > 30:\n",
    "                                titulo_elem = container\n",
    "                            else:\n",
    "                                continue\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    titulo = titulo_elem.get_text(strip=True)\n",
    "                    \n",
    "                    if len(titulo) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    # Pega URL\n",
    "                    link_elem = container.find('a', href=True) if container.name != 'a' else container\n",
    "                    url_noticia = ''\n",
    "                    if link_elem and link_elem.get('href'):\n",
    "                        href = link_elem['href']\n",
    "                        # Filtra URLs válidas\n",
    "                        if any(x in href for x in ['/page/', '/tag/', '/categoria/', '#', 'javascript:', 'mailto:']):\n",
    "                            continue\n",
    "                        \n",
    "                        if href.startswith('/'):\n",
    "                            url_noticia = 'https://www.moneytimes.com.br' + href\n",
    "                        elif 'moneytimes.com.br' in href:\n",
    "                            url_noticia = href\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    if not url_noticia:\n",
    "                        continue\n",
    "                    \n",
    "                    # Pega data/hora\n",
    "                    data_hora = ''\n",
    "                    parent = container.parent if container.parent else container\n",
    "                    \n",
    "                    # Procura elemento time\n",
    "                    time_elem = parent.find('time')\n",
    "                    if time_elem:\n",
    "                        data_hora = time_elem.get_text(strip=True)\n",
    "                    else:\n",
    "                        # Procura por classe date\n",
    "                        date_elem = parent.find(class_='date')\n",
    "                        if date_elem:\n",
    "                            data_hora = date_elem.get_text(strip=True)\n",
    "                        else:\n",
    "                            # Busca por padrão de tempo no texto\n",
    "                            texto = parent.get_text()\n",
    "                            match = re.search(r'\\\\d+\\\\s*(hora|dia|minuto)s?\\\\s*(atrás|atras)', texto, re.IGNORECASE)\n",
    "                            if match:\n",
    "                                data_hora = match.group(0)\n",
    "                    \n",
    "                    # Pega lead\n",
    "                    lead = ''\n",
    "                    p_elem = container.find('p')\n",
    "                    if p_elem:\n",
    "                        lead = p_elem.get_text(strip=True)\n",
    "                    \n",
    "                    # Verifica duplicatas\n",
    "                    if not any(n['url'] == url_noticia for n in noticias):\n",
    "                        noticias.append({\n",
    "                            'titulo': titulo,\n",
    "                            'data_hora': data_hora,\n",
    "                            'url': url_noticia,\n",
    "                            'lead': lead,\n",
    "                            'data_extracao': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        })\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"  ✅ Extraídas {len(noticias)} notícias\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Erro: {str(e)}\")\n",
    "    \n",
    "    return noticias\n",
    "\n",
    "print(\"Função extrair_noticias_pagina() criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Função Principal: Coletar 110 Notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Função coletar_noticias() criada!\n"
     ]
    }
   ],
   "source": [
    "def coletar_noticias(numero_noticias=110):\n",
    "    \"\"\"\n",
    "    Coleta exatamente 110 notícias do Giro do Mercado.\n",
    "    Não para até coletar todas as notícias solicitadas!\n",
    "    \n",
    "    Args:\n",
    "        numero_noticias: Número de notícias a coletar (padrão: 110)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame do pandas com as notícias\n",
    "    \"\"\"\n",
    "    driver = setup_driver()\n",
    "    todas_noticias = []\n",
    "    pagina = 1\n",
    "    max_paginas = 20  # Limite de segurança\n",
    "    paginas_vazias = 0\n",
    "    \n",
    "    try:\n",
    "        print(\"=\"*80)\n",
    "        print(f\" INICIANDO COLETA DE {numero_noticias} NOTÍCIAS - GIRO DO MERCADO\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\" URL: https://www.moneytimes.com.br/tag/giro-do-mercado/\")\n",
    "        print(f\"  Estimativa: ~{numero_noticias // 10 + 1} páginas (~10 notícias por página)\")\n",
    "        print(f\"  O processo NÃO PARA até coletar {numero_noticias} notícias!\\\\n\")\n",
    "        \n",
    "        while len(todas_noticias) < numero_noticias and pagina <= max_paginas:\n",
    "            # Monta URL\n",
    "            if pagina == 1:\n",
    "                url = 'https://www.moneytimes.com.br/tag/giro-do-mercado/'\n",
    "            else:\n",
    "                url = f'https://www.moneytimes.com.br/tag/giro-do-mercado/page/{pagina}/'\n",
    "            \n",
    "            print(f\"\\\\n PÁGINA {pagina}: {url}\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            # Extrai notícias\n",
    "            noticias_pagina = extrair_noticias_pagina(driver, url)\n",
    "            \n",
    "            if not noticias_pagina:\n",
    "                paginas_vazias += 1\n",
    "                print(f\"  Página vazia ({paginas_vazias} consecutivas)\")\n",
    "                \n",
    "                if paginas_vazias >= 3:\n",
    "                    print(f\" Muitas páginas vazias. Continuando busca...\")\n",
    "                \n",
    "                pagina += 1\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                paginas_vazias = 0\n",
    "            \n",
    "            # Adiciona notícias únicas\n",
    "            noticias_novas = 0\n",
    "            for noticia in noticias_pagina:\n",
    "                if not any(n['url'] == noticia['url'] for n in todas_noticias):\n",
    "                    todas_noticias.append(noticia)\n",
    "                    noticias_novas += 1\n",
    "                    \n",
    "                    if len(todas_noticias) >= numero_noticias:\n",
    "                        break\n",
    "            \n",
    "            # Mostra progresso\n",
    "            faltam = numero_noticias - len(todas_noticias)\n",
    "            porcentagem = (len(todas_noticias) / numero_noticias) * 100\n",
    "            \n",
    "            print(f\"  {noticias_novas} notícias novas adicionadas\")\n",
    "            print(f\"  Progresso: {len(todas_noticias)}/{numero_noticias} ({porcentagem:.1f}%)\")\n",
    "            \n",
    "            if faltam > 0:\n",
    "                print(f\"  Faltam: {faltam} notícias\")\n",
    "            else:\n",
    "                print(f\"  META ATINGIDA!\")\n",
    "                break\n",
    "            \n",
    "            pagina += 1\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Resultado final\n",
    "        print(\"\\\\n\" + \"=\"*80)\n",
    "        if len(todas_noticias) >= numero_noticias:\n",
    "            print(\" COLETA FINALIZADA COM SUCESSO! \")\n",
    "        else:\n",
    "            print(\"  COLETA ENCERRADA (limite de páginas atingido)\")\n",
    "        \n",
    "        print(f\" Total de notícias: {len(todas_noticias)}\")\n",
    "        print(f\" Páginas navegadas: {pagina}\")\n",
    "        print(f\"  Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*80 + \"\\\\n\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\\\n\\\\n Coleta interrompida pelo usuário!\")\n",
    "        print(f\" Notícias coletadas até agora: {len(todas_noticias)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n\\\\n✗ ERRO: {str(e)}\")\n",
    "        print(f\" Notícias coletadas até o erro: {len(todas_noticias)}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\\\n WebDriver encerrado.\")\n",
    "    \n",
    "    # Cria DataFrame\n",
    "    df = pd.DataFrame(todas_noticias)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\" Função coletar_noticias() criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Executar Coleta\n",
    "\n",
    "**ATENÇÃO:** Execute esta célula para iniciar a coleta das 110 notícias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:11:40,647 - INFO - ====== WebDriver manager ======\n",
      "2025-10-25 23:11:41,648 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-10-25 23:11:41,973 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-10-25 23:11:42,286 - INFO - Driver [C:\\Users\\wemed\\.wdm\\drivers\\chromedriver\\win64\\141.0.7390.122\\chromedriver-win32/chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " INICIANDO COLETA DE 110 NOTÍCIAS - GIRO DO MERCADO\n",
      "================================================================================\n",
      " URL: https://www.moneytimes.com.br/tag/giro-do-mercado/\n",
      "  Estimativa: ~12 páginas (~10 notícias por página)\n",
      "  O processo NÃO PARA até coletar 110 notícias!\\n\n",
      "\\n PÁGINA 1: https://www.moneytimes.com.br/tag/giro-do-mercado/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 10/110 (9.1%)\n",
      "  Faltam: 100 notícias\n",
      "\\n PÁGINA 2: https://www.moneytimes.com.br/tag/giro-do-mercado/page/2/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 20/110 (18.2%)\n",
      "  Faltam: 90 notícias\n",
      "\\n PÁGINA 3: https://www.moneytimes.com.br/tag/giro-do-mercado/page/3/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 30/110 (27.3%)\n",
      "  Faltam: 80 notícias\n",
      "\\n PÁGINA 4: https://www.moneytimes.com.br/tag/giro-do-mercado/page/4/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 40/110 (36.4%)\n",
      "  Faltam: 70 notícias\n",
      "\\n PÁGINA 5: https://www.moneytimes.com.br/tag/giro-do-mercado/page/5/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 50/110 (45.5%)\n",
      "  Faltam: 60 notícias\n",
      "\\n PÁGINA 6: https://www.moneytimes.com.br/tag/giro-do-mercado/page/6/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 60/110 (54.5%)\n",
      "  Faltam: 50 notícias\n",
      "\\n PÁGINA 7: https://www.moneytimes.com.br/tag/giro-do-mercado/page/7/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 70/110 (63.6%)\n",
      "  Faltam: 40 notícias\n",
      "\\n PÁGINA 8: https://www.moneytimes.com.br/tag/giro-do-mercado/page/8/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 80/110 (72.7%)\n",
      "  Faltam: 30 notícias\n",
      "\\n PÁGINA 9: https://www.moneytimes.com.br/tag/giro-do-mercado/page/9/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 90/110 (81.8%)\n",
      "  Faltam: 20 notícias\n",
      "\\n PÁGINA 10: https://www.moneytimes.com.br/tag/giro-do-mercado/page/10/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 100/110 (90.9%)\n",
      "  Faltam: 10 notícias\n",
      "\\n PÁGINA 11: https://www.moneytimes.com.br/tag/giro-do-mercado/page/11/\n",
      "--------------------------------------------------------------------------------\n",
      "  Encontrados 11 elementos 'news-item'\n",
      "  ✅ Extraídas 10 notícias\n",
      "  10 notícias novas adicionadas\n",
      "  Progresso: 110/110 (100.0%)\n",
      "  META ATINGIDA!\n",
      "\\n================================================================================\n",
      " COLETA FINALIZADA COM SUCESSO! \n",
      " Total de notícias: 110\n",
      " Páginas navegadas: 11\n",
      "  Data/Hora: 2025-10-25 23:12:50\n",
      "================================================================================\\n\n",
      "\\n WebDriver encerrado.\n",
      "\\n DataFrame criado com 110 notícias!\n",
      " Colunas: ['titulo', 'data_hora', 'url', 'lead', 'data_extracao']\n"
     ]
    }
   ],
   "source": [
    "# EXECUTAR COLETA\n",
    "df_noticias = coletar_noticias(110)\n",
    "\n",
    "print(f\"\\\\n DataFrame criado com {len(df_noticias)} notícias!\")\n",
    "print(f\" Colunas: {list(df_noticias.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análises Adicionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>data_hora</th>\n",
       "      <th>url</th>\n",
       "      <th>lead</th>\n",
       "      <th>data_extracao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPCA-15 abaixo do esperado reforça cenário pos...</td>\n",
       "      <td>1 dia(s) atrás</td>\n",
       "      <td>https://www.moneytimes.com.br/ipca-15-abaixo-d...</td>\n",
       "      <td>Giro do MercadoIPCA-15 abaixo do esperado refo...</td>\n",
       "      <td>2025-10-25 23:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Petróleo dispara com tensões geopolíticas e an...</td>\n",
       "      <td>2 dia(s) atrás</td>\n",
       "      <td>https://www.moneytimes.com.br/petroleo-dispara...</td>\n",
       "      <td>Giro do MercadoPetróleo dispara com tensões ge...</td>\n",
       "      <td>2025-10-25 23:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mercado se anima com Vale (VALE3) e Weg (WEGE3...</td>\n",
       "      <td>3 dia(s) atrás</td>\n",
       "      <td>https://www.moneytimes.com.br/mercado-se-anima...</td>\n",
       "      <td>Giro do MercadoMercado se anima com Vale (VALE...</td>\n",
       "      <td>2025-10-25 23:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analista avalia drama da Ambipar (AMBP3) — ‘Fo...</td>\n",
       "      <td>4 dia(s) atrás</td>\n",
       "      <td>https://www.moneytimes.com.br/analista-avalia-...</td>\n",
       "      <td>Giro do MercadoAnalista avalia drama da Ambipa...</td>\n",
       "      <td>2025-10-25 23:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inflação menor reforça aposta em corte da Seli...</td>\n",
       "      <td>5 dia(s) atrás</td>\n",
       "      <td>https://www.moneytimes.com.br/inflacao-menor-r...</td>\n",
       "      <td>Giro do MercadoInflação menor reforça aposta e...</td>\n",
       "      <td>2025-10-25 23:11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Risco fiscal segue no radar do mercado; analis...</td>\n",
       "      <td>30 mai 2025</td>\n",
       "      <td>https://www.moneytimes.com.br/fiscal-volta-a-p...</td>\n",
       "      <td>Giro do MercadoRisco fiscal segue no radar do ...</td>\n",
       "      <td>2025-10-25 23:12:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Mercados reagem a suspensão das tarifas de Tru...</td>\n",
       "      <td>29 mai 2025</td>\n",
       "      <td>https://www.moneytimes.com.br/mercados-reagem-...</td>\n",
       "      <td>Giro do MercadoMercados reagem a suspensão das...</td>\n",
       "      <td>2025-10-25 23:12:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Ibovespa recua em dia de recuperação judicial ...</td>\n",
       "      <td>28 mai 2025</td>\n",
       "      <td>https://www.moneytimes.com.br/ibovespa-recua-e...</td>\n",
       "      <td>Giro do MercadoIbovespa recua em dia de recupe...</td>\n",
       "      <td>2025-10-25 23:12:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Inflação desacelera e impulsiona Ibovespa; ana...</td>\n",
       "      <td>27 mai 2025</td>\n",
       "      <td>https://www.moneytimes.com.br/inflacao-desacel...</td>\n",
       "      <td>Giro do MercadoInflação desacelera e impulsion...</td>\n",
       "      <td>2025-10-25 23:12:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Ibovespa tem leve alta com Trump e Focus no ra...</td>\n",
       "      <td>26 mai 2025</td>\n",
       "      <td>https://www.moneytimes.com.br/ibovespa-repercu...</td>\n",
       "      <td>Giro do MercadoIbovespa tem leve alta com Trum...</td>\n",
       "      <td>2025-10-25 23:12:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                titulo       data_hora  \\\n",
       "0    IPCA-15 abaixo do esperado reforça cenário pos...  1 dia(s) atrás   \n",
       "1    Petróleo dispara com tensões geopolíticas e an...  2 dia(s) atrás   \n",
       "2    Mercado se anima com Vale (VALE3) e Weg (WEGE3...  3 dia(s) atrás   \n",
       "3    Analista avalia drama da Ambipar (AMBP3) — ‘Fo...  4 dia(s) atrás   \n",
       "4    Inflação menor reforça aposta em corte da Seli...  5 dia(s) atrás   \n",
       "..                                                 ...             ...   \n",
       "105  Risco fiscal segue no radar do mercado; analis...     30 mai 2025   \n",
       "106  Mercados reagem a suspensão das tarifas de Tru...     29 mai 2025   \n",
       "107  Ibovespa recua em dia de recuperação judicial ...     28 mai 2025   \n",
       "108  Inflação desacelera e impulsiona Ibovespa; ana...     27 mai 2025   \n",
       "109  Ibovespa tem leve alta com Trump e Focus no ra...     26 mai 2025   \n",
       "\n",
       "                                                   url  \\\n",
       "0    https://www.moneytimes.com.br/ipca-15-abaixo-d...   \n",
       "1    https://www.moneytimes.com.br/petroleo-dispara...   \n",
       "2    https://www.moneytimes.com.br/mercado-se-anima...   \n",
       "3    https://www.moneytimes.com.br/analista-avalia-...   \n",
       "4    https://www.moneytimes.com.br/inflacao-menor-r...   \n",
       "..                                                 ...   \n",
       "105  https://www.moneytimes.com.br/fiscal-volta-a-p...   \n",
       "106  https://www.moneytimes.com.br/mercados-reagem-...   \n",
       "107  https://www.moneytimes.com.br/ibovespa-recua-e...   \n",
       "108  https://www.moneytimes.com.br/inflacao-desacel...   \n",
       "109  https://www.moneytimes.com.br/ibovespa-repercu...   \n",
       "\n",
       "                                                  lead        data_extracao  \n",
       "0    Giro do MercadoIPCA-15 abaixo do esperado refo...  2025-10-25 23:11:49  \n",
       "1    Giro do MercadoPetróleo dispara com tensões ge...  2025-10-25 23:11:49  \n",
       "2    Giro do MercadoMercado se anima com Vale (VALE...  2025-10-25 23:11:49  \n",
       "3    Giro do MercadoAnalista avalia drama da Ambipa...  2025-10-25 23:11:49  \n",
       "4    Giro do MercadoInflação menor reforça aposta e...  2025-10-25 23:11:49  \n",
       "..                                                 ...                  ...  \n",
       "105  Giro do MercadoRisco fiscal segue no radar do ...  2025-10-25 23:12:50  \n",
       "106  Giro do MercadoMercados reagem a suspensão das...  2025-10-25 23:12:50  \n",
       "107  Giro do MercadoIbovespa recua em dia de recupe...  2025-10-25 23:12:50  \n",
       "108  Giro do MercadoInflação desacelera e impulsion...  2025-10-25 23:12:50  \n",
       "109  Giro do MercadoIbovespa tem leve alta com Trum...  2025-10-25 23:12:50  \n",
       "\n",
       "[110 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza todas as notícias\n",
    "df_noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-validate-md",
   "metadata": {},
   "source": [
    "## 8. Validação e Normalização de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-validate-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:13:24,856 - INFO - Iniciando validação de 110 registros\n",
      "2025-10-25 23:13:24,865 - INFO - Validação concluída: 110 registros processados\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " ETAPA 1: VALIDAÇÃO E NORMALIZAÇÃO DE DADOS\n",
      "================================================================================\n",
      "[LOG] Total de registros antes da validação: 110\n",
      "[VALIDAÇÃO] Verificando dados...\n",
      "  ✓ URLs válidas: 110/110\n",
      "  ✓ Títulos válidos: 110/110\n",
      "  ✓ Datas de extração válidas: 110/110\n",
      "[NORMALIZAÇÃO] Normalizando campos...\n",
      "  ✓ Títulos normalizados\n",
      "  ✓ URLs normalizadas\n",
      "  ✓ Leads normalizados\n",
      "  ✓ Datas convertidas (relativas → absolutas)\n",
      "[ESTATÍSTICAS]\n",
      "  • Registros válidos: 110 de 110\n",
      "  • Comprimento médio do título: 99.1 caracteres\n",
      "  • Comprimento médio do lead: 111.2 caracteres\n",
      "✓ Validação e normalização concluídas!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configurar logs\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print('='*80)\n",
    "print(' ETAPA 1: VALIDAÇÃO E NORMALIZAÇÃO DE DADOS')\n",
    "print('='*80)\n",
    "\n",
    "logger.info(f'Iniciando validação de {len(df_noticias)} registros')\n",
    "print(f'[LOG] Total de registros antes da validação: {len(df_noticias)}')\n",
    "\n",
    "# ===== VALIDAÇÕES =====\n",
    "def validar_url(url):\n",
    "    if not url:\n",
    "        return False\n",
    "    try:\n",
    "        resultado = urlparse(url)\n",
    "        return all([resultado.scheme, resultado.netloc])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def validar_titulo(titulo):\n",
    "    return isinstance(titulo, str) and len(titulo.strip()) >= 10\n",
    "\n",
    "def validar_data_extracao(data_str):\n",
    "    try:\n",
    "        datetime.strptime(data_str, '%Y-%m-%d %H:%M:%S')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# ===== NORMALIZAÇÃO =====\n",
    "def normalizar_titulo(titulo):\n",
    "    return ' '.join(titulo.split())\n",
    "\n",
    "def normalizar_url(url):\n",
    "    if not url:\n",
    "        return None\n",
    "    return url.split('#')[0].strip()\n",
    "\n",
    "def normalizar_lead(lead):\n",
    "    if not lead:\n",
    "        return ''\n",
    "    lead = re.sub(r'^Giro\\s+do\\s+Mercado\\s*', '', lead, flags=re.IGNORECASE)\n",
    "    return ' '.join(lead.split())\n",
    "\n",
    "def extrair_data_relativa_para_datetime(data_relativa):\n",
    "    if not data_relativa:\n",
    "        return None\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)\\s*(hora|dia|minuto)s?\\s*(atrás|atras)', data_relativa, re.IGNORECASE)\n",
    "        if match:\n",
    "            quantidade = int(match.group(1))\n",
    "            unidade = match.group(2).lower()\n",
    "            agora = datetime.now()\n",
    "            if 'hora' in unidade:\n",
    "                data = agora - timedelta(hours=quantidade)\n",
    "            elif 'dia' in unidade:\n",
    "                data = agora - timedelta(days=quantidade)\n",
    "            elif 'minuto' in unidade:\n",
    "                data = agora - timedelta(minutes=quantidade)\n",
    "            else:\n",
    "                return None\n",
    "            return data.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        match = re.search(r'(\\d+)\\s+(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)\\s+(\\d{4})', data_relativa, re.IGNORECASE)\n",
    "        if match:\n",
    "            meses = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6,\n",
    "                     'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}\n",
    "            dia = int(match.group(1))\n",
    "            mes = meses.get(match.group(2).lower(), 1)\n",
    "            ano = int(match.group(3))\n",
    "            try:\n",
    "                data = datetime(ano, mes, dia, 10, 0, 0)\n",
    "                return data.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.warning(f'Erro ao converter data: {e}')\n",
    "        return None\n",
    "\n",
    "df_validado = df_noticias.copy()\n",
    "\n",
    "print('[VALIDAÇÃO] Verificando dados...')\n",
    "\n",
    "urls_validas = df_validado['url'].apply(validar_url)\n",
    "print(f'  ✓ URLs válidas: {urls_validas.sum()}/{len(df_validado)}')\n",
    "\n",
    "titulos_validos = df_validado['titulo'].apply(validar_titulo)\n",
    "print(f'  ✓ Títulos válidos: {titulos_validos.sum()}/{len(df_validado)}')\n",
    "\n",
    "datas_validas = df_validado['data_extracao'].apply(validar_data_extracao)\n",
    "print(f'  ✓ Datas de extração válidas: {datas_validas.sum()}/{len(df_validado)}')\n",
    "\n",
    "print('[NORMALIZAÇÃO] Normalizando campos...')\n",
    "\n",
    "df_validado['titulo'] = df_validado['titulo'].apply(normalizar_titulo)\n",
    "df_validado['url'] = df_validado['url'].apply(normalizar_url)\n",
    "df_validado['lead'] = df_validado['lead'].apply(normalizar_lead)\n",
    "df_validado['data_publicacao'] = df_validado['data_hora'].apply(extrair_data_relativa_para_datetime)\n",
    "\n",
    "print(f'  ✓ Títulos normalizados')\n",
    "print(f'  ✓ URLs normalizadas')\n",
    "print(f'  ✓ Leads normalizados')\n",
    "print(f'  ✓ Datas convertidas (relativas → absolutas)')\n",
    "\n",
    "print('[ESTATÍSTICAS]')\n",
    "print(f'  • Registros válidos: {urls_validas.sum()} de {len(df_validado)}')\n",
    "print(f'  • Comprimento médio do título: {df_validado[\"titulo\"].str.len().mean():.1f} caracteres')\n",
    "print(f'  • Comprimento médio do lead: {df_validado[\"lead\"].str.len().mean():.1f} caracteres')\n",
    "\n",
    "logger.info(f'Validação concluída: {len(df_validado)} registros processados')\n",
    "print('✓ Validação e normalização concluídas!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-dedupe-md",
   "metadata": {},
   "source": [
    "## 9. Deduplicacao de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-dedupe-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:13:32,558 - INFO - Iniciando deduplicacao de 110 registros\n",
      "2025-10-25 23:13:32,565 - INFO - Deduplicacao concluida: 110 registros unicos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " ETAPA 2: DEDUPLICACAO DE DADOS\n",
      "================================================================================\n",
      "[LOG] Total de registros antes da deduplicacao: 110\n",
      "[HASH] Hashes gerados para deduplicacao\n",
      "[DEDUPLICACAO] Estrategia 1: Por URL (Primary Key)\n",
      "  URLs duplicadas encontradas: 0\n",
      "  Registros apos deduplicacao por URL: 110\n",
      "[DEDUPLICACAO] Estrategia 2: Por Hash (Chave Composta URL + Titulo)\n",
      "  Chaves compostas duplicadas: 0\n",
      "  Registros apos deduplicacao por hash: 110\n",
      "[ANALISE] Verificando titulos semelhantes\n",
      "  Titulos identicos (hashes iguais): 0\n",
      "[ESTATISTICAS FINAIS]\n",
      "  Registros iniciais: 110\n",
      "  Registros apos validacao: 110\n",
      "  Duplicatas removidas: 0\n",
      "  Taxa de deduplicacao: 0.00%\n",
      "Deduplicacao concluida!\n"
     ]
    }
   ],
   "source": [
    "print('='*80)\n",
    "print(' ETAPA 2: DEDUPLICACAO DE DADOS')\n",
    "print('='*80)\n",
    "\n",
    "logger.info(f'Iniciando deduplicacao de {len(df_validado)} registros')\n",
    "print(f'[LOG] Total de registros antes da deduplicacao: {len(df_validado)}')\n",
    "\n",
    "def gerar_hash_noticia(url, titulo):\n",
    "    chave_composta = f\"{url}|{titulo}\".lower()\n",
    "    return hashlib.sha256(chave_composta.encode()).hexdigest()\n",
    "\n",
    "def gerar_hash_titulo(titulo):\n",
    "    return hashlib.md5(titulo.lower().encode()).hexdigest()\n",
    "\n",
    "df_validado['hash_noticia'] = df_validado.apply(\n",
    "    lambda row: gerar_hash_noticia(row['url'], row['titulo']),\n",
    "    axis=1\n",
    ")\n",
    "df_validado['hash_titulo'] = df_validado['titulo'].apply(gerar_hash_titulo)\n",
    "\n",
    "print('[HASH] Hashes gerados para deduplicacao')\n",
    "\n",
    "print('[DEDUPLICACAO] Estrategia 1: Por URL (Primary Key)')\n",
    "duplicatas_url = df_validado.duplicated(subset=['url'], keep=False)\n",
    "print(f'  URLs duplicadas encontradas: {duplicatas_url.sum()}')\n",
    "\n",
    "if duplicatas_url.sum() > 0:\n",
    "    df_validado = df_validado.drop_duplicates(subset=['url'], keep='first')\n",
    "    logger.info(f'Removidas duplicatas por URL')\n",
    "\n",
    "print(f'  Registros apos deduplicacao por URL: {len(df_validado)}')\n",
    "\n",
    "print('[DEDUPLICACAO] Estrategia 2: Por Hash (Chave Composta URL + Titulo)')\n",
    "duplicatas_hash = df_validado.duplicated(subset=['hash_noticia'], keep=False)\n",
    "print(f'  Chaves compostas duplicadas: {duplicatas_hash.sum()}')\n",
    "\n",
    "if duplicatas_hash.sum() > 0:\n",
    "    df_validado = df_validado.drop_duplicates(subset=['hash_noticia'], keep='first')\n",
    "    logger.info(f'Removidas duplicatas por chave composta')\n",
    "\n",
    "print(f'  Registros apos deduplicacao por hash: {len(df_validado)}')\n",
    "\n",
    "print('[ANALISE] Verificando titulos semelhantes')\n",
    "duplicatas_titulo = df_validado['hash_titulo'].duplicated(keep=False).sum()\n",
    "print(f'  Titulos identicos (hashes iguais): {duplicatas_titulo // 2}')\n",
    "\n",
    "print('[ESTATISTICAS FINAIS]')\n",
    "print(f'  Registros iniciais: {len(df_noticias)}')\n",
    "print(f'  Registros apos validacao: {len(df_validado)}')\n",
    "duplicatas_removidas = len(df_noticias) - len(df_validado)\n",
    "print(f'  Duplicatas removidas: {duplicatas_removidas}')\n",
    "if len(df_noticias) > 0:\n",
    "    print(f'  Taxa de deduplicacao: {(duplicatas_removidas/len(df_noticias)*100):.2f}%')\n",
    "\n",
    "df_validado = df_validado.reset_index(drop=True)\n",
    "\n",
    "logger.info(f'Deduplicacao concluida: {len(df_validado)} registros unicos')\n",
    "print('Deduplicacao concluida!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-duckdb-md",
   "metadata": {},
   "source": [
    "## 10. Modelagem em Banco Analitico (DuckDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-duckdb-code",
   "metadata": {},
   "outputs": [],
   "source": "import duckdb\nimport os\nimport yfinance as yf\nfrom datetime import timedelta\n\nprint('='*80)\nprint(' ETAPA 3: MODELAGEM EM BANCO ANALITICO (DuckDB)')\nprint('='*80)\n\n# Caminho do banco: pasta onde o notebook está sendo executado\ndb_path = os.path.join(os.getcwd(), 'giro_mercado.duckdb')\n\nconn = duckdb.connect(db_path)\nlogger.info(f'Conexao DuckDB estabelecida: {db_path}')\nprint(f'\\n[LOG] Banco de dados criado/conectado: {db_path}')\n\nprint('\\n[TABELA 1] Criando tabela tb_noticias')\n\ndf_para_db = df_validado.copy()\ndf_para_db['id_noticia'] = range(1, len(df_para_db) + 1)\ndf_para_db['ano'] = pd.to_datetime(df_para_db['data_publicacao'], errors='coerce').dt.year\ndf_para_db['mes'] = pd.to_datetime(df_para_db['data_publicacao'], errors='coerce').dt.month\ndf_para_db['dia'] = pd.to_datetime(df_para_db['data_publicacao'], errors='coerce').dt.day\n\ndf_tb_noticias = df_para_db[[\n    'id_noticia', 'titulo', 'url', 'lead', \n    'data_publicacao', 'data_extracao', 'hash_noticia',\n    'ano', 'mes', 'dia'\n]]\n\ntry:\n    conn.execute('DROP TABLE IF EXISTS tb_noticias')\n    conn.execute('CREATE TABLE tb_noticias AS SELECT * FROM df_tb_noticias')\n    logger.info(f'Tabela tb_noticias criada com {len(df_tb_noticias)} registros')\n    print(f'  Tabela tb_noticias criada com {len(df_tb_noticias)} registros')\nexcept Exception as e:\n    logger.error(f'Erro ao criar tb_noticias: {e}')\n    print(f'  Erro: {e}')\n\nprint('\\n[TABELA 2] Criando tabela tb_metricas')\n\nmetricas_list = []\nfor idx, row in df_para_db.iterrows():\n    metricas_list.append({\n        'id_noticia': row['id_noticia'],\n        'comprimento_titulo': len(row['titulo']) if row['titulo'] else 0,\n        'comprimento_lead': len(row['lead']) if row['lead'] else 0,\n        'num_palavras_titulo': len(row['titulo'].split()) if row['titulo'] else 0,\n        'num_palavras_lead': len(row['lead'].split()) if row['lead'] else 0,\n        'tem_url': 1 if row['url'] else 0,\n        'data_calculo': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    })\n\ndf_tb_metricas = pd.DataFrame(metricas_list)\n\ntry:\n    conn.execute('DROP TABLE IF EXISTS tb_metricas')\n    conn.execute('CREATE TABLE tb_metricas AS SELECT * FROM df_tb_metricas')\n    logger.info(f'Tabela tb_metricas criada com {len(df_tb_metricas)} registros')\n    print(f'  Tabela tb_metricas criada com {len(df_tb_metricas)} registros')\nexcept Exception as e:\n    logger.error(f'Erro ao criar tb_metricas: {e}')\n    print(f'  Erro: {e}')\n\nprint('\\n[TABELA 3] Buscando dados do Bitcoin (Últimos 6 meses) via yfinance')\n\n# Busca dados históricos do Bitcoin dos últimos 6 meses\ntry:\n    # Data final: hoje\n    data_fim = datetime.now()\n    # Data inicial: 6 meses atrás\n    data_inicio = data_fim - timedelta(days=180)\n    \n    data_inicio_str = data_inicio.strftime('%Y-%m-%d')\n    data_fim_str = data_fim.strftime('%Y-%m-%d')\n    \n    print(f'  Buscando dados do Bitcoin de {data_inicio_str} ate {data_fim_str} (últimos 6 meses)')\n    \n    # Baixa dados do Bitcoin (BTC-USD)\n    btc_data = yf.download('BTC-USD', start=data_inicio_str, end=data_fim_str, progress=False)\n    \n    if not btc_data.empty:\n        # Cria dicionário de preços por data\n        btc_prices = {}\n        for date, row in btc_data.iterrows():\n            data_str = date.strftime('%Y-%m-%d')\n            btc_prices[data_str] = round(row['Close'], 2)\n        \n        print(f'  Dados do Bitcoin coletados: {len(btc_prices)} dias de histórico')\n        logger.info(f'Dados Bitcoin coletados: {len(btc_prices)} registros')\n        \n        # Cria DataFrame com histórico completo de Bitcoin\n        bitcoin_historico = []\n        for idx, (data, preco) in enumerate(sorted(btc_prices.items())):\n            data_dt = pd.to_datetime(data)\n            bitcoin_historico.append({\n                'id_bitcoin': idx + 1,\n                'data': data,\n                'preco': preco,\n                'ano': data_dt.year,\n                'mes': data_dt.month,\n                'dia': data_dt.day\n            })\n        \n        df_historico_bitcoin = pd.DataFrame(bitcoin_historico)\n        print(f'  Tabela de histórico Bitcoin: {len(df_historico_bitcoin)} registros')\n        \n    else:\n        btc_prices = {}\n        df_historico_bitcoin = pd.DataFrame()\n        print(f'  Aviso: Nenhum dado do Bitcoin foi retornado')\n        logger.warning('Nenhum dado do Bitcoin retornado pelo yfinance')\n        \nexcept Exception as e:\n    btc_prices = {}\n    df_historico_bitcoin = pd.DataFrame()\n    logger.error(f'Erro ao buscar dados do Bitcoin: {e}')\n    print(f'  Erro ao buscar Bitcoin: {e}')\n\nprint('\\n[TABELA 4] Criando tabela tb_ativo (com dados de Bitcoin)')\n\n# Cria tabela ativo com a coluna bitcoin preenchida\nativo_list = []\nfor idx, row in df_para_db.iterrows():\n    # Busca preço do Bitcoin para a data da notícia\n    bitcoin_price = None\n    if pd.notna(row['data_publicacao']):\n        data_noticia = pd.to_datetime(row['data_publicacao']).strftime('%Y-%m-%d')\n        bitcoin_price = btc_prices.get(data_noticia)\n    \n    ativo_list.append({\n        'id_ativo': idx + 1,\n        'id_noticia': row['id_noticia'],\n        'titulo': row['titulo'],\n        'url': row['url'],\n        'data_publicacao': row['data_publicacao'],\n        'data_extracao': row['data_extracao'],\n        'bitcoin': bitcoin_price,  # Preço do Bitcoin no fechamento do dia\n        'ano': row['ano'],\n        'mes': row['mes'],\n        'dia': row['dia']\n    })\n\ndf_tb_ativo = pd.DataFrame(ativo_list)\n\n# Estatísticas do Bitcoin\nbitcoin_preenchidos = df_tb_ativo['bitcoin'].notna().sum()\nprint(f'  Registros com preço do Bitcoin: {bitcoin_preenchidos}/{len(df_tb_ativo)}')\n\ntry:\n    conn.execute('DROP TABLE IF EXISTS tb_ativo')\n    conn.execute('CREATE TABLE tb_ativo AS SELECT * FROM df_tb_ativo')\n    logger.info(f'Tabela tb_ativo criada com {len(df_tb_ativo)} registros')\n    print(f'  Tabela tb_ativo criada com {len(df_tb_ativo)} registros')\nexcept Exception as e:\n    logger.error(f'Erro ao criar tb_ativo: {e}')\n    print(f'  Erro: {e}')\n\nprint('\\n[TABELA 5] Criando tabela tb_bitcoin_historico (Últimos 6 meses)')\n\n# Cria tabela com histórico completo de Bitcoin\nif not df_historico_bitcoin.empty:\n    try:\n        conn.execute('DROP TABLE IF EXISTS tb_bitcoin_historico')\n        conn.execute('CREATE TABLE tb_bitcoin_historico AS SELECT * FROM df_historico_bitcoin')\n        logger.info(f'Tabela tb_bitcoin_historico criada com {len(df_historico_bitcoin)} registros')\n        print(f'  Tabela tb_bitcoin_historico criada com {len(df_historico_bitcoin)} registros de histórico')\n    except Exception as e:\n        logger.error(f'Erro ao criar tb_bitcoin_historico: {e}')\n        print(f'  Erro: {e}')\nelse:\n    print(f'  Tabela tb_bitcoin_historico não criada (sem dados disponíveis)')\n\nprint('\\n[TABELA 6] Criando tabela tb_auditoria')\n\nauditoria_list = [{\n    'id_execucao': hashlib.sha256(datetime.now().isoformat().encode()).hexdigest()[:16],\n    'data_execucao': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'total_noticias': len(df_tb_noticias),\n    'total_metricas': len(df_tb_metricas),\n    'total_ativo': len(df_tb_ativo),\n    'bitcoin_preenchidos': int(bitcoin_preenchidos),\n    'total_bitcoin_historico': len(df_historico_bitcoin) if not df_historico_bitcoin.empty else 0,\n    'periodo_bitcoin': f'{data_inicio_str} a {data_fim_str}' if 'data_inicio_str' in locals() else 'N/A',\n    'origem_dados': 'Money Times - Giro do Mercado',\n    'fonte_bitcoin': 'yfinance (BTC-USD) - Últimos 6 meses',\n    'status': 'SUCESSO'\n}]\n\ndf_tb_auditoria = pd.DataFrame(auditoria_list)\n\ntry:\n    conn.execute('DROP TABLE IF EXISTS tb_auditoria')\n    conn.execute('CREATE TABLE tb_auditoria AS SELECT * FROM df_tb_auditoria')\n    logger.info(f'Tabela tb_auditoria criada com {len(df_tb_auditoria)} registros')\n    print(f'  Tabela tb_auditoria criada com {len(df_tb_auditoria)} registros')\nexcept Exception as e:\n    logger.error(f'Erro ao criar tb_auditoria: {e}')\n    print(f'  Erro: {e}')\n\nprint('\\n[ESTRUTURA DO BANCO]')\ntry:\n    tabelas = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_type = 'BASE TABLE'\").fetchall()\n    print(f'  Tabelas criadas no banco: {len(tabelas)}')\n    for tabela in tabelas:\n        table_name = tabela[0]\n        count = conn.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]\n        print(f'    {table_name}: {count} registros')\nexcept Exception as e:\n    logger.error(f'Erro ao listar tabelas: {e}')\n\nlogger.info('Modelagem em DuckDB concluida com sucesso')\nprint(f'\\nBanco de dados DuckDB configurado com sucesso!')\nprint(f'  Caminho: {os.path.abspath(db_path)}')\nprint(f'  Período de dados de Bitcoin: Últimos 6 meses ({data_inicio_str} a {data_fim_str})')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-md",
   "metadata": {},
   "source": [
    "## 11. Resumo Executivo - Pipeline ETL Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary-code",
   "metadata": {},
   "outputs": [],
   "source": "print('='*80)\nprint(' RESUMO EXECUTIVO - PIPELINE ETL COMPLETO')\nprint('='*80)\n\ntempo_final = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\nprint(f'\\n[EXECUCAO] Data/Hora: {tempo_final}')\nprint(f'[ARQUIVO] Notebook: scraping_giro_mercado.ipynb')\nprint(f'[BANCO] Arquivo DuckDB: giro_mercado.duckdb (pasta atual)')\n\nprint('\\n[ESTATISTICAS GERAIS]')\nprint('='*80)\n\nprint('\\n1. EXTRACAO DE DADOS')\nprint(f'  Noticias coletadas: {len(df_noticias)}')\nprint(f'  Fonte: Money Times - Giro do Mercado')\nprint(f'  Periodo de cobertura: Ultimos ~3 meses de historico')\nprint(f'  URL Base: https://www.moneytimes.com.br/tag/giro-do-mercado/')\nprint(f'  Campos coletados: 5 (titulo, data/hora, URL, lead, data_extracao)')\n\nprint('\\n2. VALIDACAO DE DADOS')\nprint(f'  Registros validados: {len(df_validado)}')\n# Recalcula as validações para esta célula\nurls_validas_resumo = df_validado['url'].apply(lambda x: bool(x) and len(str(x)) > 10)\ntitulos_validos_resumo = df_validado['titulo'].apply(lambda x: isinstance(x, str) and len(x.strip()) >= 10)\ndatas_validas_resumo = df_validado['data_extracao'].notna()\n\nprint(f'  URLs validas: {urls_validas_resumo.sum()}/{len(df_validado)}')\nprint(f'  Titulos validos: {titulos_validos_resumo.sum()}/{len(df_validado)}')\nprint(f'  Datas validas: {datas_validas_resumo.sum()}/{len(df_validado)}')\nif len(df_noticias) > 0:\n    print(f'  Taxa de validacao: {(len(df_validado)/len(df_noticias)*100):.2f}%')\n\nprint('\\n3. DEDUPLICACAO')\nprint(f'  Registros unicos: {len(df_validado)}')\nprint(f'  Duplicatas removidas: {len(df_noticias) - len(df_validado)}')\nif len(df_noticias) > 0:\n    print(f'  Taxa de deduplicacao: {((len(df_noticias) - len(df_validado))/len(df_noticias)*100):.2f}%')\nprint(f'  Estrategia: Hash (Chave Composta URL + Titulo) + URL Primary Key')\n\nprint('\\n4. BANCO ANALITICO (DuckDB)')\nprint(f'  Tabelas criadas: 5')\nprint(f'    - tb_noticias: {len(df_tb_noticias)} registros (tabela principal)')\nprint(f'    - tb_metricas: {len(df_tb_metricas)} registros (metricas de conteudo)')\nprint(f'    - tb_ativo: {len(df_tb_ativo)} registros (dados de ativos com precos de Bitcoin)')\nprint(f'    - tb_bitcoin_historico: {len(df_historico_bitcoin) if not df_historico_bitcoin.empty else 0} registros (histórico completo dos últimos 6 meses)')\nprint(f'    - tb_auditoria: {len(df_tb_auditoria)} registros (rastreabilidade)')\nprint(f'  Relacionamentos: Foreign Keys (id_noticia, id_bitcoin)')\nprint(f'  Arquivo: {os.path.abspath(db_path)}')\nif os.path.exists(db_path):\n    tamanho_kb = os.path.getsize(db_path) / 1024\n    print(f'  Tamanho do arquivo: {tamanho_kb:.2f} KB')\n\nprint('\\n5. DADOS DE BITCOIN (ULTIMOS 6 MESES)')\nbitcoin_preenchidos = df_tb_ativo['bitcoin'].notna().sum()\nprint(f'  Registros com preco do Bitcoin em tb_ativo: {bitcoin_preenchidos}/{len(df_tb_ativo)}')\nbitcoin_historico_count = len(df_historico_bitcoin) if not df_historico_bitcoin.empty else 0\nprint(f'  Registros de histórico em tb_bitcoin_historico: {bitcoin_historico_count} dias')\nif bitcoin_historico_count > 0:\n    # Converte coluna preco para valores numéricos\n    bitcoin_preco_numeric = pd.to_numeric(df_historico_bitcoin['preco'], errors='coerce')\n    bitcoin_min = bitcoin_preco_numeric.min()\n    bitcoin_max = bitcoin_preco_numeric.max()\n    bitcoin_media = bitcoin_preco_numeric.mean()\n    data_min_btc = df_historico_bitcoin['data'].min()\n    data_max_btc = df_historico_bitcoin['data'].max()\n    print(f'  Periodo de dados: {data_min_btc} a {data_max_btc}')\n    print(f'  Preco minimo BTC: ${bitcoin_min:,.2f}')\n    print(f'  Preco maximo BTC: ${bitcoin_max:,.2f}')\n    print(f'  Preco medio BTC: ${bitcoin_media:,.2f}')\nprint(f'  Fonte: yfinance (BTC-USD)')\n\nprint('\\n6. ANALISE DO CONTEUDO')\nprint(f'  Comprimento medio do titulo: {df_validado[\"titulo\"].str.len().mean():.1f} caracteres')\nprint(f'  Comprimento medio do lead: {df_validado[\"lead\"].str.len().mean():.1f} caracteres')\n\ntry:\n    datas_validas_dt = pd.to_datetime(df_validado['data_publicacao'], errors='coerce')\n    data_min = datas_validas_dt.min()\n    data_max = datas_validas_dt.max()\n    dias_cobertura = (data_max - data_min).days if pd.notna(data_min) and pd.notna(data_max) else 0\n    \n    print(f'  Data minima dos dados: {data_min.strftime(\"%Y-%m-%d\") if pd.notna(data_min) else \"N/A\"}')\n    print(f'  Data maxima dos dados: {data_max.strftime(\"%Y-%m-%d\") if pd.notna(data_max) else \"N/A\"}')\n    print(f'  Cobertura temporal: {dias_cobertura} dias')\n    if dias_cobertura >= 90:\n        print(f'  Status: SERIE HISTORICA >= 3 MESES')\n    else:\n        print(f'  Status: AVISO - Serie historica < 3 meses')\nexcept Exception as e:\n    print(f'  Erro ao calcular periodo: {e}')\n    dias_cobertura = 0\n\nprint('\\n7. DADOS ESTRUTURADOS E QUALIDADE')\nprint(f'  Formato de saida: DataFrame Pandas + DuckDB')\nprint(f'  Tipos de dados normalizados:')\nprint(f'    - Titulos: string (normalizado)')\nprint(f'    - URLs: string (validadas)')\nprint(f'    - Datas: datetime (convertidas de relativas para absolutas)')\nprint(f'    - Leads: string (limpas e normalizadas)')\nprint(f'    - Bitcoin: float (preco de fechamento em USD)')\nprint(f'  Indices unicos: hash_noticia (SHA256)')\nprint(f'  Validacoes implementadas: 3 (URL, Titulo, Data)')\n\nprint('\\n8. DEPENDENCIAS (requirements.txt)')\nprint(f'  Versoes fixas especificadas:')\ndependencies = {\n    'Web Scraping': ['selenium==4.15.2', 'beautifulsoup4==4.12.2', 'webdriver-manager==4.0.1', 'lxml==4.9.3'],\n    'Data Processing': ['pandas==2.1.3', 'numpy==1.26.2'],\n    'Financial Data': ['yfinance==0.2.32'],\n    'Database': ['duckdb==0.9.2'],\n    'Utilities': ['python-dateutil==2.8.2', 'requests==2.31.0']\n}\n\nfor categoria, libs in dependencies.items():\n    print(f'    {categoria}:')\n    for lib in libs:\n        print(f'      {lib}')\n\nprint('\\n' + '='*80)\nprint(' PIPELINE ETL RESUMIDO')\nprint('='*80)\nprint('''\\nETAPA 1: EXTRACAO\n  Coleta de 110+ noticias via Selenium + BeautifulSoup\n  Estrutura: Titulo, Data/Hora, URL, Lead, Data Extracao\n\nETAPA 2: TRANSFORMACAO\n  Validacao (URLs, titulos, datas)\n  Normalizacao (trim, conversao de datas relativas)\n  Deduplicacao (hash SHA256, URL primary key)\n  Enriquecimento (calculo de metricas, dados de Bitcoin via yfinance)\n\nETAPA 3: CARGA\n  DuckDB Analytics Database (salvo na pasta atual)\n  Tabelas relacionadas: noticias, metricas, ativo, bitcoin_historico (NOVO!), auditoria\n  Índices e JOINs otimizados para análise\n  Tabela tb_ativo inclui coluna bitcoin com preços reais de fechamento (BTC-USD)\n  Tabela tb_bitcoin_historico inclui histórico completo de 6 meses de Bitcoin\n\nETAPA 4: QUALIDADE\n  Logs de execucao em tempo real\n  Rastreabilidade completa (tb_auditoria)\n  Estatisticas de transformacao\n  Integracao com API financeira (yfinance) - Últimos 6 meses\n''')\n\nprint('\\n' + '='*80)\nprint(' STATUS FINAL')\nprint('='*80)\n\nstatus_checks = {\n    'DADOS ESTRUTURADOS E SERIES HISTORICAS >= 3 MESES': dias_cobertura >= 90,\n    'VALIDACOES DE DADOS IMPLEMENTADAS': len(df_validado) > 0,\n    'DEDUPLICACAO COM ESTRATEGIA ROBUSTA': len(df_validado) <= len(df_noticias),\n    'MODELAGEM EM DUCKDB (5 TABELAS)': len(df_tb_noticias) > 0 and len(df_tb_metricas) > 0,\n    'HISTORICO DE BITCOIN (ULTIMOS 6 MESES)': bitcoin_historico_count > 0,\n    'DADOS DE BITCOIN INTEGRADOS (YFINANCE)': bitcoin_preenchidos > 0,\n    'BANCO SALVO NA PASTA ATUAL': os.path.exists(db_path),\n    'REQUIREMENTS.TXT COM VERSOES FIXAS': os.path.exists('requirements.txt'),\n    'LOGS DE EXECUCAO DETALHADOS': True,\n}\n\nfor check, status in status_checks.items():\n    simbolo = 'OK' if status else 'FALHA'\n    print(f'  [{simbolo}] {check}')\n\nprint('\\n' + '='*80)\nlogger.info('Pipeline ETL concluido com sucesso!')\nprint(' PIPELINE ETL CONCLUIDO COM SUCESSO!')\nprint('='*80)\n\nconn.close()\nlogger.info('Conexao DuckDB fechada')\nprint('\\nTodos os processos finalizados com sucesso!')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}